# TRG 算法中 χ 增大時誤差飽和現象的深度分析

## 核心問題

在 TRG (Tensor Renormalization Group) 算法中，理論上增大 bond dimension χ 應該能提升精度，但實際觀察到：

- **χ = 4 → 8**：誤差顯著下降（~50% 改善）
- **χ = 8 → 16**：誤差略微下降（~10-20% 改善）
- **χ = 16 → 32**：幾乎無改善（<5%）
- **χ = 32 → 64**：完全飽和（<1%）

這種「邊際收益遞減」現象背後有深層的數學和數值原因。

---

## 原因一：幾何限制（Geometric Limitation）

### 1.1 SVD 維度上限

對於 rank-4 張量 `T[u,r,d,l]`，每個維度為 `d`，進行 SVD 分解時：

```
T[u,r,d,l] → reshape((u,r), (d,l)) → shape (d², d²)
SVD → U, S, Vh
```

- **理論最大奇異值數量**：`min(d², d²) = d²`
- **初始張量**：`d=2`，最多 4 個奇異值
- **第一次粗粒化後**：`d=4`，最多 16 個奇異值
- **第二次粗粒化後**：`d=16`，最多 256 個奇異值

### 1.2 早期步驟的瓶頸效應

參考 `chi_limitation_analysis.py` 中的發現：

```
Step 1: 只有 4 個奇異值  → 設定 χ=64 也只能用到 4
Step 2: 有 16 個奇異值  → χ>16 被幾何限制
Step 3: 有 64 個奇異值  → 但大部分 <1e-12（見下節）
```

**關鍵結論**：即使最終步驟能產生大量奇異值，早期步驟的「信息瓶頸」已經限制了最終精度。

---

## 原因二：奇異值快速衰減（Singular Value Decay）

### 2.1 物理信息的集中性

2D Ising 模型在臨界點附近的物理信息高度集中在**低頻模式**（對應大奇異值）：

```
S[0] ≈ 1.0         ← 主導模式（>80% 信息）
S[1] ≈ 0.3         ← 次主導模式（~15% 信息）
S[2] ≈ 0.05        ← 修正項（~4% 信息）
S[3] ≈ 0.01        ← 小修正（~1% 信息）
S[4] ≈ 1e-4        ← 高頻噪聲
S[5+] < 1e-8       ← 數值噪聲
```

### 2.2 累積能量佔比

定義能量佔比：`E(k) = Σ(S[0:k]²) / Σ(S²)`

典型數值（Step 3-4）：

- `E(4) = 99.9%`  → χ=4 已捕獲絕大部分信息
- `E(8) = 99.99%` → χ=8 捕獲額外 0.09%
- `E(16) = 99.9999%` → χ=16 僅再增加 0.0099%
- `E(32)` 與 `E(16)` 差異 < 0.0001%（低於數值精度）

**數學本質**：2D Ising 的關聯函數在空間上快速衰減，對應頻域的奇異值快速衰減。

---

## 原因三：數值精度限制（Numerical Precision）

### 3.1 浮點數精度

- **IEEE 754 雙精度 (float64)**：
  - 機器精度：`ε ≈ 2.22e-16`
  - 可靠有效數字：~15-16 位

### 3.2 矩陣運算的誤差累積

TRG 每步涉及：

1. **SVD 分解**：數值誤差 ~ `O(ε × ||T||)`
2. **張量收縮**：4 次 einsum，每次累積誤差
3. **歸一化**：對數運算引入相對誤差

**累積效應**：經過 15-20 次迭代後，總誤差約 `10⁻¹² ~ 10⁻¹⁰`

### 3.3 極小奇異值的放大效應

當保留 `S[k] < 1e-12` 的奇異值時：

```
截斷誤差 ≈ ||丟棄的奇異值||² ≈ Σ(S[k>χ]²)
```

- 若 `S[k] ~ 1e-12`，其平方 ~ `1e-24` （遠低於浮點精度）
- 但 SVD 分解誤差本身 ~ `1e-15 × S[0]`

**矛盾**：極小奇異值的「物理貢獻」低於「數值誤差」，保留它們反而降低精度。

---

## 原因四：算法固有誤差（Algorithmic Limitation）

### 4.1 TRG 的近似性質

TRG 是**近似粗粒化方法**，存在兩類誤差：

1. **截斷誤差**：丟棄小奇異值引入
2. **重整化誤差**：2×2 合併不完全保持物理對稱性

### 4.2 誤差累積的飽和

定義第 `n` 步的總誤差：

```
E_total(n) = E_truncation(n) + E_renormalization(n) + E_numerical(n)
```

- **早期**（n < 5）：`E_truncation` 主導（χ 的影響大）
- **中期**（5 < n < 15）：三者平衡
- **後期**（n > 15）：`E_numerical` 主導（χ 無法改善）

**關鍵公式**（來自 `trg_final_project.py:251`）：

```python
f = -T * Σ_n [ln(g_n) / N_n]
```

每步的 `ln(g_n)` 累積誤差，當迭代次數足夠多時，這些小誤差累加到與截斷誤差相當的量級。

---

## 實驗驗證

### 測試設置

- **溫度**：`T = Tc = 2/ln(1+√2) ≈ 2.269`
- **迭代次數**：20
- **χ 範圍**：2, 4, 8, 16, 32, 64, 128

### 預期結果（基於 `test_chi_effect.py`）

| χ   | 相對誤差 (%) | 相比前一 χ 改善 | 計算時間 (相對) |
|-----|-------------|----------------|----------------|
| 2   | ~5.0        | --             | 1×             |
| 4   | ~2.5        | 50%            | 2×             |
| 8   | ~1.2        | 52%            | 8×             |
| 16  | ~1.0        | 17%            | 32×            |
| 32  | ~0.95       | 5%             | 128×           |
| 64  | ~0.93       | 2%             | 512×           |
| 128 | ~0.92       | <1%            | 2048×          |

### 關鍵觀察

1. **χ=8** 是性價比最優點：
   - 誤差已降至 ~1%（接近算法極限）
   - 計算成本僅為 χ=64 的 1/64

2. **χ>16** 進入飽和區：
   - 誤差改善 <5%
   - 計算成本指數增長（SVD 複雜度 O(χ³)）

3. **χ>64** 完全無意義：
   - 改善幅度 <1%（低於統計波動）
   - 受數值精度絕對限制

---

## 數學直覺

### 類比：傅立葉級數

TRG 的奇異值分解類似於函數的傅立葉展開：

```
f(x) = Σ a_n × e^(inx)
```

- **低頻項**（n=0,1,2）：捕獲主要形狀
- **中頻項**（n=3~10）：捕獲細節
- **高頻項**（n>10）：噪聲，保留反而降低逼近精度

**對應關係**：
- 大奇異值 ↔ 低頻模式（物理信息）
- 小奇異值 ↔ 高頻模式（數值噪聲）
- χ ↔ 保留的頻率數

---

## 突破瓶頸的方法

若確實需要更高精度，可考慮：

### 1. 算法改進

- **HOTRG** (Higher-Order TRG)：
  - 改善重整化誤差
  - χ=16 可達到 TRG χ=64 的精度

- **TNR** (Tensor Network Renormalization)：
  - 顯式優化等距性
  - 進一步降低算法誤差

### 2. 數值技術

- **任意精度算術**（如 `mpmath`）：
  - 突破 float64 限制
  - 計算成本增加 100-1000 倍

- **混合精度**：
  - 關鍵步驟用 `float128`
  - 平衡精度與效率

### 3. 物理洞察

- **對稱性利用**：
  - 顯式保持 Z₂ 對稱性
  - 減少有效自由度

---

## 結論

χ 增大時誤差飽和是**四種效應共同作用**的結果：

1. **幾何限制**：早期步驟只有少量有效奇異值
2. **信息集中**：物理信息集中在前幾個模式
3. **數值精度**：浮點數精度限制絕對下限
4. **算法誤差**：TRG 本身存在固有近似

對於 **2D Ising @ Tc**：

- **χ = 8**：最優選擇（誤差 ~1%，效率高）
- **χ = 16**：保守選擇（誤差 ~0.9%，成本可接受）
- **χ > 32**：浪費計算資源，改善 <1%

**核心洞察**：數值算法的精度不是單調提升的，而是受物理、數學和計算三重約束。理解這些限制是設計高效算法的關鍵。

---

## 參考文獻

1. Levin & Nave (2007) - "Tensor Renormalization Group Approach to 2D Classical Lattice Models"
2. Evenbly & Vidal (2015) - "Tensor Network Renormalization"
3. 本專案代碼：
   - `trg_final_project.py` - 主要實現
   - `chi_limitation_analysis.py` - 幾何限制視覺化
   - `test_svd_threshold.py` - 數值穩定性測試
